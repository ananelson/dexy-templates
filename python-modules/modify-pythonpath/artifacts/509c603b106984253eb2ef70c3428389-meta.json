{"artifact_class_source": "class FileSystemJsonArtifact(Artifact):\n    \"\"\"Artifact which persists data by writing to the file system (default type\n    of Artifact)\"\"\"\n\n    FILTERS = Controller().find_filters()\n\n    # Metadata\n    def meta_filename(self):\n        return \"%s-meta.json\" % (self.hashstring)\n\n    def meta_filepath(self):\n        return os.path.join(self.artifacts_dir, self.meta_filename())\n\n    def save_meta(self):\n        m = {}\n        attrs_to_persist = set(self.META_ATTRS + self.HASH_WHITELIST) - set(['input_data_dict', 'inputs'])\n        for a in attrs_to_persist:\n            if hasattr(self, a):\n                v = getattr(self, a)\n                m[a] = v\n\n        m['inputs'] = {}\n        for k, a in self.inputs().iteritems():\n            a.save()\n            m['inputs'][k] = a.hashstring\n\n        f = open(self.meta_filepath(), \"w\")\n        try:\n            json.dump(m, f)\n        except UnicodeDecodeError as e:\n            print e\n            print m\n            raise Exception(\"Binary data present in %s\" % self.key)\n        f.close()\n\n    def load_meta(self):\n        f = open(self.meta_filepath(), \"r\")\n        m = json.load(f)\n        f.close()\n\n        self._inputs = dict((k, self.__class__.retrieve(h)) for (k, h) in m.pop('inputs').iteritems())\n\n        for k, v in m.iteritems():\n            setattr(self, k, v)\n\n        # We only store filter name, not filter class, need to retrieve class from name\n        if hasattr(self, \"filter_name\") and not hasattr(self, \"filter_class\"):\n            self.filter_class = [k for n,k in self.FILTERS.iteritems() if k.__name__ == self.filter_name][0]\n\n    # Input\n    def load_input(self):\n        \"\"\"Load input data into memory, if applicable.\"\"\"\n        if self.binary_input:\n            #not loading non-binary input\n            pass\n        elif self.initial:\n            #initial artifact has no input\n            pass\n        elif self.additional:\n            #additional artifact has no input\n            pass\n        elif len(self.input_data_dict) > 0:\n            #we already have input data in memory\n            pass\n        elif not hasattr(self, 'previous_cached_output_filepath'):\n            #no previous cached output, can't load\n            pass\n        else:\n            f = open(self.previous_cached_output_filepath, \"r\")\n            data_dict = json.load(f)\n            f.close()\n\n            self.input_data_dict = OrderedDict() # maybe unnecessary\n            for x in sorted(data_dict.keys()):\n                k = x.split(\":\", 1)[1]\n                self.input_data_dict[k] = data_dict[x]\n\n    # Output\n    def cached_output_filename(self):\n        return \"%s-output.json\" % (self.hashstring)\n\n    def cached_output_filepath(self):\n        return os.path.join(self.artifacts_dir, self.cached_output_filename())\n\n    def is_output_cached(self):\n        # TODO add checksums to verify data hasn't changed\n        if self.binary_output:\n            return self.is_canonical_output_cached()\n        else:\n            return self.is_json_output_cached() and self.is_canonical_output_cached()\n\n    def is_json_output_cached(self):\n        fp = self.cached_output_filepath()\n        return os.path.isfile(fp) and (os.path.getsize(fp) > 0)\n\n    def is_canonical_output_cached(self):\n        fp = self.filepath()\n        return os.path.isfile(fp) and (os.path.getsize(fp) > 0)\n\n    def save_output(self):\n        if not self.is_complete():\n            raise Exception(\"should not be calling save_output unless artifact is complete\")\n\n#        if self.is_output_cached():\n#            print \"we will be overwriting existing files\"\n\n        if not self.binary_output:\n            if not self.data_dict or len(self.data_dict) == 0:\n                # Our filter has written directly to an output file\n                # We need to load this into memory first\n                self.data_dict = OrderedDict()\n                f = open(self.filepath(), 'r')\n                data = f.read()\n                f.close()\n                self.data_dict['1'] = data\n\n            # need to preserve ordering but we can't serialize OrderedDict\n            # using JSON, so add sortable numbers to keys to preserve order\n            data_dict = {}\n            MAX = 10000\n            if len(self.data_dict) >= MAX:\n                raise Exception(\"\"\"There is an arbitrary limit of %s dict items,\n                               you can increase this if you need to.\"\"\" % MAX)\n            i = -1\n            for k, v in self.data_dict.iteritems():\n                i += 1\n                data_dict[\"%04d:%s\" % (i, k)] = v\n\n            # Write the JSON file.\n            f = open(self.cached_output_filepath(), \"w\")\n            try:\n                json.dump(data_dict, f)\n            except UnicodeDecodeError as e:\n                print e\n                print self.binary_output\n                raise Exception(self.key)\n\n            f.close()\n\n            # Write the canonical file.\n            f = open(self.filepath(), 'w')\n            f.write(self.output_text())\n            f.close()\n\n    def load_output(self):\n        if not self.is_complete():\n            raise Exception(\"should not be calling load_output unless artifact is complete\")\n\n        if not self.binary_output:\n            f = open(self.cached_output_filepath(), \"r\")\n            data_dict = json.load(f)\n            f.close()\n\n            self.data_dict = OrderedDict() # maybe unnecessary\n            for x in sorted(data_dict.keys()):\n                k = x.split(\":\", 1)[1]\n                self.data_dict[k] = data_dict[x]\n", "binary_output": false, "args": {}, "additional": null, "name": "demomodule.py", "inputs": {}, "binary_input": false, "initial": true, "ext": ".py", "elapsed": 0, "state": "complete", "dirty": false, "key": "demomodule.py", "dexy_version": "0.4.0", "final": null, "next_filter_name": "PygHandler", "document_key": "demomodule.py|pyg"}