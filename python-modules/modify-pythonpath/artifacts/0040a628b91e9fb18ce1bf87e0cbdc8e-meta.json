{"artifact_class_source": "class FileSystemJsonArtifact(Artifact):\n    \"\"\"Artifact which persists data by writing to the file system (default type\n    of Artifact)\"\"\"\n\n    FILTERS = Controller().find_filters()\n\n    # Metadata\n    def meta_filename(self):\n        return \"%s-meta.json\" % (self.hashstring)\n\n    def meta_filepath(self):\n        return os.path.join(self.artifacts_dir, self.meta_filename())\n\n    def save_meta(self):\n        m = {}\n        attrs_to_persist = set(self.META_ATTRS + self.HASH_WHITELIST) - set(['input_data_dict', 'inputs'])\n        for a in attrs_to_persist:\n            if hasattr(self, a):\n                v = getattr(self, a)\n                m[a] = v\n\n        m['inputs'] = {}\n        for k, a in self.inputs().iteritems():\n            a.save()\n            m['inputs'][k] = a.hashstring\n\n        f = open(self.meta_filepath(), \"w\")\n        try:\n            json.dump(m, f)\n        except UnicodeDecodeError as e:\n            print e\n            print m\n            raise Exception(\"Binary data present in %s\" % self.key)\n        f.close()\n\n    def load_meta(self):\n        f = open(self.meta_filepath(), \"r\")\n        m = json.load(f)\n        f.close()\n\n        self._inputs = dict((k, self.__class__.retrieve(h)) for (k, h) in m.pop('inputs').iteritems())\n\n        for k, v in m.iteritems():\n            setattr(self, k, v)\n\n        # We only store filter name, not filter class, need to retrieve class from name\n        if hasattr(self, \"filter_name\") and not hasattr(self, \"filter_class\"):\n            self.filter_class = [k for n,k in self.FILTERS.iteritems() if k.__name__ == self.filter_name][0]\n\n    # Input\n    def load_input(self):\n        \"\"\"Load input data into memory, if applicable.\"\"\"\n        if self.binary_input:\n            #not loading non-binary input\n            pass\n        elif self.initial:\n            #initial artifact has no input\n            pass\n        elif self.additional:\n            #additional artifact has no input\n            pass\n        elif len(self.input_data_dict) > 0:\n            #we already have input data in memory\n            pass\n        elif not hasattr(self, 'previous_cached_output_filepath'):\n            #no previous cached output, can't load\n            pass\n        else:\n            f = open(self.previous_cached_output_filepath, \"r\")\n            data_dict = json.load(f)\n            f.close()\n\n            self.input_data_dict = OrderedDict() # maybe unnecessary\n            for x in sorted(data_dict.keys()):\n                k = x.split(\":\", 1)[1]\n                self.input_data_dict[k] = data_dict[x]\n\n    # Output\n    def cached_output_filename(self):\n        return \"%s-output.json\" % (self.hashstring)\n\n    def cached_output_filepath(self):\n        return os.path.join(self.artifacts_dir, self.cached_output_filename())\n\n    def is_output_cached(self):\n        # TODO add checksums to verify data hasn't changed\n        if self.binary_output:\n            return self.is_canonical_output_cached()\n        else:\n            return self.is_json_output_cached() and self.is_canonical_output_cached()\n\n    def is_json_output_cached(self):\n        fp = self.cached_output_filepath()\n        return os.path.isfile(fp) and (os.path.getsize(fp) > 0)\n\n    def is_canonical_output_cached(self):\n        fp = self.filepath()\n        return os.path.isfile(fp) and (os.path.getsize(fp) > 0)\n\n    def save_output(self):\n        if not self.is_complete():\n            raise Exception(\"should not be calling save_output unless artifact is complete\")\n\n#        if self.is_output_cached():\n#            print \"we will be overwriting existing files\"\n\n        if not self.binary_output:\n            if not self.data_dict or len(self.data_dict) == 0:\n                # Our filter has written directly to an output file\n                # We need to load this into memory first\n                self.data_dict = OrderedDict()\n                f = open(self.filepath(), 'r')\n                data = f.read()\n                f.close()\n                self.data_dict['1'] = data\n\n            # need to preserve ordering but we can't serialize OrderedDict\n            # using JSON, so add sortable numbers to keys to preserve order\n            data_dict = {}\n            MAX = 10000\n            if len(self.data_dict) >= MAX:\n                raise Exception(\"\"\"There is an arbitrary limit of %s dict items,\n                               you can increase this if you need to.\"\"\" % MAX)\n            i = -1\n            for k, v in self.data_dict.iteritems():\n                i += 1\n                data_dict[\"%04d:%s\" % (i, k)] = v\n\n            # Write the JSON file.\n            f = open(self.cached_output_filepath(), \"w\")\n            try:\n                json.dump(data_dict, f)\n            except UnicodeDecodeError as e:\n                print e\n                print self.binary_output\n                raise Exception(self.key)\n\n            f.close()\n\n            # Write the canonical file.\n            f = open(self.filepath(), 'w')\n            f.write(self.output_text())\n            f.close()\n\n    def load_output(self):\n        if not self.is_complete():\n            raise Exception(\"should not be calling load_output unless artifact is complete\")\n\n        if not self.binary_output:\n            f = open(self.cached_output_filepath(), \"r\")\n            data_dict = json.load(f)\n            f.close()\n\n            self.data_dict = OrderedDict() # maybe unnecessary\n            for x in sorted(data_dict.keys()):\n                k = x.split(\":\", 1)[1]\n                self.data_dict[k] = data_dict[x]\n", "input_ext": ".pycon", "args": {}, "additional": null, "name": "demo-script.py", "binary_output": false, "binary_input": false, "initial": null, "dirty": false, "ext": ".html", "final": false, "state": "complete", "output_hash": "51b4115b22fa65f58cd49e7e7d11bf5b2a184de2cf685c6d2475a775be1cca3ddc0e8d17d35072d06e92e64e2f09cf9231245a9abb73513fcca10052c7c2a6be", "key": "demo-script.py|idio|pycon|pyg", "filter_name": "PygHandler", "dexy_version": "0.4.0", "elapsed": 0.023621082305908203, "filter_source": "class PygHandler(DexyFilter):\n    \"\"\"\n    Apply Pygments syntax highlighting.\n    \"\"\"\n    INPUT_EXTENSIONS = [\".*\"]\n    IMAGE_OUTPUT_EXTENSIONS = ['.png', '.bmp', '.gif', '.jpg', '.svg']\n    OUTPUT_EXTENSIONS = [\".html\", \".tex\"] + IMAGE_OUTPUT_EXTENSIONS\n    ALIASES = ['pyg', 'pygments']\n    FINAL = False\n\n    def process_dict(self, input_dict):\n        ext = self.artifact.input_ext\n        name = \"input_text%s\" % ext\n        # List any file extensions which don't map neatly to lexers.\n        if ext == '.pycon':\n            lexer = PythonConsoleLexer()\n        elif ext == '.rbcon':\n            lexer = RubyConsoleLexer()\n        elif ext in ('.json', '.dexy'):\n            lexer = JavascriptLexer()\n        elif ext == '.Rd':\n            lexer = TextLexer()\n        else:\n            lexer = get_lexer_for_filename(name)\n\n        if self.artifact.args.has_key('pygments'):\n            pygments_args = self.artifact.args['pygments']\n        else:\n            pygments_args = {}\n\n        formatter_args = {'lineanchors' : self.artifact.document_key }\n        if pygments_args.has_key('formatter'):\n            formatter_args.update(pygments_args['formatter'])\n\n        formatter = get_formatter_for_filename(self.artifact.filename(),\n                **formatter_args)\n\n        if self.artifact.ext in self.IMAGE_OUTPUT_EXTENSIONS:\n            self.artifact.binary_output = True\n            f = open(self.artifact.filepath(), 'w')\n            f.write(highlight(self.artifact.input_text(), lexer, formatter))\n            f.close()\n        else:\n            output_dict = OrderedDict()\n            for k, v in input_dict.items():\n                # TODO figure out where these characters are coming from and don't hard-code this.\n                v = str(v.replace(\" \\x08\", \"\").replace(chr(13), \"\"))\n                try:\n                    output_dict[k] = str(highlight(v, lexer, formatter))\n                except UnicodeEncodeError as e:\n                    self.artifact.log.warn(\"error processing section %s of file %s\" % (k, self.artifact.key))\n                    raise e\n            return output_dict\n", "filter_version": null, "inputs": {}, "document_key": "demo-script.py|idio|pycon|pyg"}