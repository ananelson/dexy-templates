{"artifact_class_source": "class FileSystemJsonArtifact(Artifact):\n    \"\"\"Artifact which persists data by writing to the file system (default type\n    of Artifact)\"\"\"\n\n    FILTERS = Controller().find_filters()\n\n    # Metadata\n    def meta_filename(self):\n        return \"%s-meta.json\" % (self.hashstring)\n\n    def meta_filepath(self):\n        return os.path.join(self.artifacts_dir, self.meta_filename())\n\n    def save_meta(self):\n        m = {}\n        attrs_to_persist = set(self.META_ATTRS + self.HASH_WHITELIST) - set(['input_data_dict', 'inputs'])\n        for a in attrs_to_persist:\n            if hasattr(self, a):\n                v = getattr(self, a)\n                m[a] = v\n\n        m['inputs'] = {}\n        for k, a in self.inputs().iteritems():\n            a.save()\n            m['inputs'][k] = a.hashstring\n\n        f = open(self.meta_filepath(), \"w\")\n        try:\n            json.dump(m, f)\n        except UnicodeDecodeError as e:\n            print e\n            print m\n            raise Exception(\"Binary data present in %s\" % self.key)\n        f.close()\n\n    def load_meta(self):\n        f = open(self.meta_filepath(), \"r\")\n        m = json.load(f)\n        f.close()\n\n        self._inputs = dict((k, self.__class__.retrieve(h)) for (k, h) in m.pop('inputs').iteritems())\n\n        for k, v in m.iteritems():\n            setattr(self, k, v)\n\n        # We only store filter name, not filter class, need to retrieve class from name\n        if hasattr(self, \"filter_name\") and not hasattr(self, \"filter_class\"):\n            self.filter_class = [k for n,k in self.FILTERS.iteritems() if k.__name__ == self.filter_name][0]\n\n    # Input\n    def load_input(self):\n        \"\"\"Load input data into memory, if applicable.\"\"\"\n        if self.binary_input:\n            #not loading non-binary input\n            pass\n        elif self.initial:\n            #initial artifact has no input\n            pass\n        elif self.additional:\n            #additional artifact has no input\n            pass\n        elif len(self.input_data_dict) > 0:\n            #we already have input data in memory\n            pass\n        elif not hasattr(self, 'previous_cached_output_filepath'):\n            #no previous cached output, can't load\n            pass\n        else:\n            f = open(self.previous_cached_output_filepath, \"r\")\n            data_dict = json.load(f)\n            f.close()\n\n            self.input_data_dict = OrderedDict() # maybe unnecessary\n            for x in sorted(data_dict.keys()):\n                k = x.split(\":\", 1)[1]\n                self.input_data_dict[k] = data_dict[x]\n\n    # Output\n    def cached_output_filename(self):\n        return \"%s-output.json\" % (self.hashstring)\n\n    def cached_output_filepath(self):\n        return os.path.join(self.artifacts_dir, self.cached_output_filename())\n\n    def is_output_cached(self):\n        # TODO add checksums to verify data hasn't changed\n        if self.binary_output:\n            return self.is_canonical_output_cached()\n        else:\n            return self.is_json_output_cached() and self.is_canonical_output_cached()\n\n    def is_json_output_cached(self):\n        fp = self.cached_output_filepath()\n        return os.path.isfile(fp) and (os.path.getsize(fp) > 0)\n\n    def is_canonical_output_cached(self):\n        fp = self.filepath()\n        return os.path.isfile(fp) and (os.path.getsize(fp) > 0)\n\n    def save_output(self):\n        if not self.is_complete():\n            raise Exception(\"should not be calling save_output unless artifact is complete\")\n\n#        if self.is_output_cached():\n#            print \"we will be overwriting existing files\"\n\n        if not self.binary_output:\n            if not self.data_dict or len(self.data_dict) == 0:\n                # Our filter has written directly to an output file\n                # We need to load this into memory first\n                self.data_dict = OrderedDict()\n                f = open(self.filepath(), 'r')\n                data = f.read()\n                f.close()\n                self.data_dict['1'] = data\n\n            # need to preserve ordering but we can't serialize OrderedDict\n            # using JSON, so add sortable numbers to keys to preserve order\n            data_dict = {}\n            MAX = 10000\n            if len(self.data_dict) >= MAX:\n                raise Exception(\"\"\"There is an arbitrary limit of %s dict items,\n                               you can increase this if you need to.\"\"\" % MAX)\n            i = -1\n            for k, v in self.data_dict.iteritems():\n                i += 1\n                data_dict[\"%04d:%s\" % (i, k)] = v\n\n            # Write the JSON file.\n            f = open(self.cached_output_filepath(), \"w\")\n            try:\n                json.dump(data_dict, f)\n            except UnicodeDecodeError as e:\n                print e\n                print self.binary_output\n                raise Exception(self.key)\n\n            f.close()\n\n            # Write the canonical file.\n            f = open(self.filepath(), 'w')\n            f.write(self.output_text())\n            f.close()\n\n    def load_output(self):\n        if not self.is_complete():\n            raise Exception(\"should not be calling load_output unless artifact is complete\")\n\n        if not self.binary_output:\n            f = open(self.cached_output_filepath(), \"r\")\n            data_dict = json.load(f)\n            f.close()\n\n            self.data_dict = OrderedDict() # maybe unnecessary\n            for x in sorted(data_dict.keys()):\n                k = x.split(\":\", 1)[1]\n                self.data_dict[k] = data_dict[x]\n", "input_ext": ".py", "args": {}, "additional": null, "name": "demomodule.py", "binary_output": false, "binary_input": false, "initial": null, "dirty": false, "ext": ".html", "final": null, "state": "complete", "output_hash": "3d0dd5cc911c585c2ce8f38c0b8849dc8cc4093a9f797d88a3562dab3a5fe40b1407c2d7250df6645db339317299366929ee4dde22969d052ee85b8558761800", "key": "demomodule.py|idio", "filter_name": "IdioHandler", "dexy_version": "0.4.0", "elapsed": 0.13617801666259766, "filter_source": "class IdioHandler(DexyFilter):\n    \"\"\"\n    Apply idiopidae to split document into sections at ### @export\n    \"section-name\" comments.\n    \"\"\"\n    INPUT_EXTENSIONS = [\".*\"]\n    OUTPUT_EXTENSIONS = [\".html\", \".tex\", \".txt\"]\n    ALIASES = ['idio', 'idiopidae']\n\n    def process_text_to_dict(self, input_text):\n        composer = Composer()\n        builder = idiopidae.parser.parse('Document', input_text + \"\\n\\0\")\n\n        ext = self.artifact.input_ext\n        name = \"input_text%s\" % ext\n\n        if self.artifact.args.has_key('pyg-lexer'):\n            lexer = get_lexer_by_name(self.artifact.args['pyg-lexer'])\n        elif ext == '.pycon':\n            lexer = PythonConsoleLexer()\n        elif ext == '.rbcon':\n            lexer = RubyConsoleLexer()\n        elif ext in ('.json', '.dexy'):\n            lexer = JavascriptLexer()\n        elif ext in ('.php'):\n            # If we are using idio, then our code will be in sections so we\n            # need to start inline with PHP. To avoid this, use pyg instead of\n            # idio. (Eventually should be able to specify lexer + options in config.)\n            lexer = PhpLexer(startinline=True)\n        else:\n            lexer = get_lexer_for_filename(name)\n\n        if self.artifact.args.has_key('idio'):\n            idio_args = self.artifact.args['idio']\n        else:\n            idio_args = {}\n\n        formatter_args = {'lineanchors' : self.artifact.document_key}\n        if idio_args.has_key('formatter'):\n            formatter_args.update(idio_args['formatter'])\n\n        formatter = get_formatter_for_filename(self.artifact.filename(),\n            **formatter_args)\n\n        output_dict = OrderedDict()\n        lineno = 1\n\n        for i, s in enumerate(builder.sections):\n            lines = builder.statements[i]['lines']\n            if len(lines) == 0:\n                next\n            if not re.match(\"^\\d+$\", s):\n                # Manually named section, the sectioning comment takes up a\n                # line, so account for this to keep line nos in sync.\n                lineno += 1\n            formatter.linenostart = lineno\n            formatted_lines = composer.format(lines, lexer, formatter)\n            output_dict[s] = formatted_lines\n            lineno += len(lines)\n\n        return output_dict\n", "filter_version": null, "inputs": {}, "document_key": "demomodule.py|idio"}